{"input":"  In line with Pomeau's conjecture about the relevance of directed percolation\n(DP) to turbulence onset\/decay in wall-bounded flows, we propose a minimal\nstochastic model dedicated to the interpretation of the spatially intermittent\nregimes observed in channel flow before its return to laminar flow. Numerical\nsimulations show that a regime with bands obliquely drifting in two stream-wise\nsymmetrical directions bifurcates into an asymmetrical regime, before\nultimately decaying to laminar flow. The model is expressed in terms of a\nprobabilistic cellular automaton evolving von Neumann neighbourhoods with\nprobabilities educed from a close examination of simulation results. It\nimplements band propagation and the two main local processes: longitudinal\nsplitting involving bands with the same orientation, and transversal splitting\ngiving birth to a daughter band with orientation opposite to that of its\nmother. The ultimate decay stage observed to display one-dimensional DP\nproperties in a two-dimensional geometry is interpreted as resulting from the\nirrelevance of lateral spreading in the single-orientation regime. The model\nalso reproduces the bifurcation restoring the symmetry upon variation of the\nprobability attached to transversal splitting, which opens the way to a study\nof the critical properties of that bifurcation, in analogy with thermodynamic\nphase transitions.\n","num_tokens":320,"num_sentence":6}
{"input":"  During the past decade, metal additive manufacturing (MAM) has experienced\nsignificant developments and gained much attention due to its ability to\nfabricate complex parts, manufacture products with functionally graded\nmaterials, minimize waste, and enable low-cost customization. Despite these\nadvantages, predicting the impact of processing parameters on the\ncharacteristics of an MAM printed clad is challenging due to the complex nature\nof MAM processes. Machine learning (ML) techniques can help connect the physics\nunderlying the process and processing parameters to the clad characteristics.\nIn this study, we introduce a hybrid approach which involves utilizing the data\nprovided by a calibrated multi-physics computational fluid dynamic (CFD) model\nand experimental research for preparing the essential big dataset, and then\nuses a comprehensive framework consisting of various ML models to predict and\nunderstand clad characteristics. We first compile an extensive dataset by\nfusing experimental data into the data generated using the developed CFD model\nfor this study. This dataset comprises critical clad characteristics, including\ngeometrical features such as width, height, and depth, labels identifying clad\nquality, and processing parameters. Second, we use two sets of processing\nparameters for training the ML models: machine setting parameters and\nphysics-aware parameters, along with versatile ML models and reliable\nevaluation metrics to create a comprehensive and scalable learning framework\nfor predicting clad geometry and quality. This framework can serve as a basis\nfor clad characteristics control and process optimization. The framework\nresolves many challenges of conventional modeling methods in MAM by solving t\nthe issue of data scarcity using a hybrid approach and introducing an\nefficient, accurate, and scalable platform for clad characteristics prediction\nand optimization.\n","num_tokens":392,"num_sentence":9}
{"input":"  Cities have recognized the local impact of small craft breweries, in many\nways altering municipal codes to make it easier to establish breweries and\nmaking them the anchor points of economic development and revitalization.\nNevertheless, we do not know the extent to which these strategies impacted\nchanges at the neighborhood level across the nation. In this chapter, we\nexamine the relationship between growth and locations of craft breweries and\nthe incidence of neighborhood change across the United States. In the first\npart of the chapter, we rely on a unique dataset of geocoded brewery locations\nthat tracks openings and closings from 2004 to the present. Using measures of\nneighborhood change often found in literature on gentrification-related topics,\nwe develop statistical models relying on census tract demographic and\nemployment data to determine the extent to which brewery locations are\nassociated with social and demographic shifts since 2000. The strongest\npredictor of whether a craft brewery opened in 2013 or later in a neighborhood\nwas the presence of a prior brewery. We do not find evidence entirely\nconsistent with the common narrative of a link between gentrification and craft\nbrewing, but we see a link between an influx of lower-to-middle income urban\ncreatives and the introduction of a craft breweries. We advocate for urban\nplanners to recognize the importance of craft breweries in neighborhood\nrevitalization while also protecting residents from potential displacement.\n","num_tokens":344,"num_sentence":8}
{"input":"  Classical definitions of locally complete intersection (l.c.i.) homomorphisms\nof commutative rings are limited to maps that are essentially of finite type,\nor flat. The concept introduced in this paper is meaningful for homomorphisms\nphi : R \\longrightarrow S of commutative noetherian rings. It is defined in\nterms of the structure of phi in a formal neighborhood of each point of Spec S.\nWe characterize the l.c.i. property by different conditions on the vanishing of\nthe Andr\\'e-Quillen homology of the R-algebra S. One of these descriptions\nestablishes a very general form of a conjecture of Quillen that was open even\nfor homomorphisms of finite type: If S has a finite resolution by flat\nR-modules and the cotangent complex \\cot SR is quasi-isomorphic to a bounded\ncomplex of flat S-modules, then phi is l.c.i. The proof uses a mixture of\nmethods from commutative algebra, differential graded homological algebra, and\nhomotopy theory. The l.c.i. property is shown to be stable under a variety of\noperations, including composition, decomposition, flat base change,\nlocalization, and completion. The present framework allows for the results to\nbe stated in proper generality; many of them are new even with classical\nassumptions. For instance, the stability of l.c.i. homomorphisms under\ndecomposition settles an open case in Fulton's treatment of orientations of\nmorphisms of schemes.\n","num_tokens":349,"num_sentence":9}
{"input":"  We investigate transport properties of a superconducting junction of many ($N\n\\ge 2$) one-dimensional quantum wires. We include the effectofelectron-electron\ninteraction within the one-dimensional quantum wire using a weak interaction\nrenormalization group procedure. Due to the proximity effect, transport across\nthe junction occurs via direct tunneling as well as via the crossed Andreev\nchannel. We find that the fixed point structure of this system is far more rich\nthan the fixed point structure of a normal metal$-$superconductor junction ($N\n= 1$), where we only have two fixed points - the fully insulating fixed point\nor the Andreev fixed point. Even a two wire (N=2)system with a superconducting\njunction i.e. a normalmetal$-$superconductor$-$normal metal structure, has\nnon-trivialfixed points with intermediate transmissions and reflections. We\nalso include electron-electron interaction induced back-scattering in the\nquantum wires in our study and hence obtain non-Luttinger liquid behaviour. It\nis interesting to note that {\\textsl{(a)}} effects due to inclusion of\nelectron-electron interaction induced back-scattering in the wire, and\n{\\textsl{(b)}} competition between the charge transport via the electron and\nhole channels across the junction, give rise to a non-monotonic behavior of\nconductance as a functionof temperature. We also find that transport across the\njunction depends on two independent interaction parameters. The first one is\ndue to the usual correlations coming from Friedel oscillations for spin-full\nelectrons giving rise to the well-known interaction parameter (${{\\alpha =\n(g_2-2g_1)\/2 \\pi \\hbar v_F}}$). The second one arises due to the scattering\ninduced by the proximity of the superconductor and is given by(${{\\alpha^\\prime\n= (g_2 + g_1)\/2 \\pi \\hbar v_F}}$).\n","num_tokens":457,"num_sentence":10}
{"input":"  Plasmon resonance in nanopatterned single layer graphene nanoribbon (SL-GNR),\ndouble layer graphene nanoribbon (DL-GNR) and triple layer graphene nanoribbon\n(TL-GNR) structures is studied both experimentally and by numerical\nsimulations. We use 'realistic' graphene samples in our experiments to identify\nthe key bottle necks in both experiments and theoretical models. The existence\nof electrical tunable plasmons in such stacked multilayer GNRs was first\nexperimentally verified by infrared microscopy. We find that the strength of\nthe plasmonic resonance increases in DL-GNR when compared to SL-GNRs. However,\nwe do not find a further such increase in TL-GNRs compared to DL-GNRs. We\ncarried out systematic full wave simulations using finite element technique to\nvalidate and fit experimental results, and extract the carrier scattering rate\nas a fitting parameter. The numerical simulations show remarkable agreement\nwith experiments for unpatterned SLG sheet, and a qualitative agreement for\npatterned graphene sheet. We believe that further improvements such as\nintroducing a bandgap into the numerical model could lead to a better\nquantitative agreement of numerical simulations with experiments. We also note\nthat such advanced modeling would first require better quality graphene samples\nand accurate measurements.\n","num_tokens":315,"num_sentence":9}
{"input":"  The new high energy data coming mainly from the Fermi and Swift satellites\nand from the ground based Cerenkov telescopes are making possible to study not\nonly the energetics of blazar jets, but also their connection to the associated\naccretion disks. Furthermore, the black hole mass of the most powerful objects\ncan be constrained through IR-optical emission, originating in the accretion\ndisks. For the first time, we can evaluate jet and accretion powers in units of\nthe Eddington luminosity for a large number of blazars. Firsts results are\nintriguing. Blazar jets have powers comparable to, and often larger than the\nluminosity produced by their accretion disk. Blazar jets are produced at all\naccretion rates (in Eddington units), and their appearance depends if the\naccretion regime is radiatively efficient or not. The jet power is dominated by\nthe bulk motion of matter, not by the Poynting flux, at least in the jet region\nwhere the bulk of the emission is produced, at ~1000 Schwarzschild radii. The\nmechanism at the origin of relativistic jets must be very efficient, possibly\nmore than accretion, even if accretion must play a crucial role. Black hole\nmasses for the most powerful jets at redshift ~3 exceed one billion solar\nmasses, flagging the existence of a very large population of heavy black holes\nat these redshifts.\n","num_tokens":339,"num_sentence":9}
{"input":"  We have investigated the role that the transverse electric field of the laser\nplays in the acceleration of electrons in a laser wakefield accelerator (LWFA)\noperating in the quasi-blowout regime through particle-in-cell code\nsimulations. In order to ensure that longitudinal compression and\/or transverse\nfocusing of the laser pulse is not needed before the wake can self-trap the\nplasma electrons, we have employed the ionization injection technique.\nFurthermore, the plasma density is varied such that at the lowest densities,\nthe laser pulse occupies only a fraction of the first wavelength of the wake\noscillation (the accelerating bucket), whereas at the highest density, the same\nduration laser pulse fills the entire first bucket. Although the trapped\nelectrons execute betatron oscillations due to the ion column in all cases, at\nthe lowest plasma density they do not interact with the laser field and the\nenergy gain is all due to the longitudinal wakefield. However, as the density\nis increased, there can be a significant contribution to the maximum energy due\nto direct laser acceleration (DLA) of those electrons that undergo betatron\nmotion in the plane of the polarization of the laser pulse. Eventually, DLA can\nbe the dominant energy gain mechanism over acceleration due to the longitudinal\nfield at the highest densities.\n","num_tokens":321,"num_sentence":6}
{"input":"  We give a new proof for the existence of rotationally symmetric steady and\nexpanding gradient Ricci solitons in dimension $n+1$, $2\\le n\\le 4$, with\nmetric $g=\\frac{da^2}{h(a^2)}+a^2d\\,\\sigma$ for some function $h$ where\n$d\\sigma$ is the standard metric on the unit sphere $S^n$ in $\\mathbb{R}^n$.\nMore precisely for any $\\lambda\\ge 0$, $2\\le n\\le 4$ and $\\mu_1\\in\\mathbb{R}$,\nwe prove the existence of unique solution $h\\in C^2((0,\\infty))\\cap\nC^1([0,\\infty))$ for the equation\n$2r^2h(r)h_{rr}(r)=(n-1)h(r)(h(r)-1)+rh_r(r)(rh_r(r)-\\lambda r-(n-1))$,\n$h(r)>0$, in $(0,\\infty)$ satisfying $h(0)=1$, $h_r(0)=\\mu_1$. We also prove\nthe existence of unique analytic solution of the about equation on $[0,\\infty)$\nfor any $\\lambda\\ge 0$, $n\\ge 2$ and $\\mu_1\\in\\mathbb{R}$. Moreover we will\nprove the asymptotic behaviour of the solution $h$ for any $n\\ge 2$,\n$\\lambda\\ge 0$ and $\\mu_1\\in\\mathbb{R}\\setminus\\{0\\}$.\n","num_tokens":352,"num_sentence":4}
{"input":"  Deep neural networks are vulnerable to adversarial attacks. Among different\nattack settings, the most challenging yet the most practical one is the\nhard-label setting where the attacker only has access to the hard-label output\n(prediction label) of the target model. Previous attempts are neither effective\nenough in terms of attack success rate nor efficient enough in terms of query\ncomplexity under the widely used $L_\\infty$ norm threat model. In this paper,\nwe present the Ray Searching attack (RayS), which greatly improves the\nhard-label attack effectiveness as well as efficiency. Unlike previous works,\nwe reformulate the continuous problem of finding the closest decision boundary\ninto a discrete problem that does not require any zeroth-order gradient\nestimation. In the meantime, all unnecessary searches are eliminated via a fast\ncheck step. This significantly reduces the number of queries needed for our\nhard-label attack. Moreover, interestingly, we found that the proposed RayS\nattack can also be used as a sanity check for possible \"falsely robust\" models.\nOn several recently proposed defenses that claim to achieve the\nstate-of-the-art robust accuracy, our attack method demonstrates that the\ncurrent white-box\/black-box attacks could still give a false sense of security\nand the robust accuracy drop between the most popular PGD attack and RayS\nattack could be as large as $28\\%$. We believe that our proposed RayS attack\ncould help identify falsely robust models that beat most white-box\/black-box\nattacks.\n","num_tokens":348,"num_sentence":10}
{"input":"  The localization of the repeating fast radio burst (FRB), FRB 121102,\nsuggests that it is associated with a persistent radio-luminous compact source\nin the FRB host galaxy. Using the FIRST radio catalog, I present a search for\nluminous persistent sources in nearby galaxies, with radio luminosities >10% of\nthe FRB 121102 persistent source luminosity. The galaxy sample contains about\n30% of the total galaxy g-band luminosity within <108 Mpc, in a footprint of\n10,600 deg^2. After rejecting sources likely due to active galactic nuclei\nactivity or background sources, I remain with 11 candidates that are presumably\nassociated with galactic disks or star formation regions. At least some of\nthese candidates are likely to be due to chance alignment. In addition, I find\n85 sources within 1\" of galactic nuclei. Assuming the radio persistent sources\nare not related to galactic nuclei and that they follow the galaxy g-band\nlight, the 11 sources imply a 95% confidence upper limit on the space density\nof luminous persistent sources of <5x10^-5 Mpc^-3, and that at any given time\nonly a small fraction of galaxies host a radio luminous persistent source\n(<10^-3 L_*^-1). Assuming persistent sources life time of 100 yr, this implies\na birth rate of <5x10^-7 yr^-1 Mpc^-3. Given the FRB volumetric rate, and\nassuming that all FRBs repeat and are associated with persistent radio sources,\nthis sets a lower limit on the rate of FRB events per persistent source of >0.8\nyr^-1. I argue that these 11 candidates are good targets for FRB searches and I\nestimate the FRB event rate from these candidates.\n","num_tokens":444,"num_sentence":10}
{"input":"  We present an application of unsupervised Machine Learning Clustering to the\nPAU Survey of galaxy spectral energy distribution (SED) within the COSMOS\nfield. The clustering algorithm is implemented and optimized to get the\nrelevant groups in the data SEDs. We find 12 groups from a total number of\n5,234 targets in the survey at $0.01 <$ z $< 0.28$. Among the groups, 3,545\ngalaxies (68\\%) show emission lines in the SEDs. These groups also include\n1,689 old galaxies with no active star formation. We have fitted the SED to\nevery single galaxy in each group with CIGALE. The mass, age and specific star\nformation rates (sSFR) of the galaxies range from $0.15 <$ age\/Gyr $< 11$; $6\n<$ log (M$_{\\star}$\/M$_{\\odot}$) $< 11.26$, and $-14.67 <$ log (sSFR\/yr\n$^{-1}$) $< -8$. The groups are well defined in their properties with galaxies\nhaving clear emission lines also having lower mass, are younger and have higher\nsSFR than those with elliptical like patterns. The characteristic values of\ngalaxies showing clear emission lines are in agreement with the literature for\nstarburst galaxies in COSMOS and GOODS-N fields at low redshift. The\nstar-forming main sequence, sSFR vs. stellar mass and UVJ diagram show clearly\nthat different groups fall into different regions with some overlap among\ngroups. Our main result is that the joint of low-resolution (R $\\sim$ 50)\nphotometric spectra provided by the PAU survey together with the unsupervised\nclassification provides an excellent way to classify galaxies. Moreover, it\nhelps to find and extend the analysis of extreme ELGs to lower masses and lower\nSFRs in the local Universe.\n","num_tokens":462,"num_sentence":12}
{"input":"  The outer gap model is used here to explain the spectrum and the energy\ndependent light curves of the X-ray and soft gamma-ray radiations of the\nspin-down powered pulsar PSR B1509-58.In the outer gap model, most pairs inside\nthe gap are created around the null charge surface and the gap's electric field\nseparates the two charges to move in opposite directions. Consequently, the\nregion from the null charge surface to the light cylinder is dominated by the\noutflow of particles and that from the null charge surface to the star is\ndominated by the inflow of particles. The inflow and outflow of particles move\nalong the magnetic field lines and emit curvature photons, and the incoming\ncurvature photons are converted to pairs by the strong magnetic field of the\nstar. These pairs emit synchrotron photons. We suggest that the X-rays and soft\ngamma-rays of PSR B1509-58 result from the synchrotron radiation of these\npairs, and the viewing angle of PSR B1509-58 only receives the inflow\nradiation. The magnetic pair creation requires a large pitch angle, which makes\nthe pulse profile of the synchrotron radiation distinct from that of the\ncurvature radiation. We carefully trace the pulse profiles of the synchrotron\nradiation with different pitch angles. We find that the differences between the\nlight curves of different energy bands are due to the different pitch angles of\nthe secondary pairs, and the second peak appearing at E>10MeV comes from the\nregion near the star, where the stronger magnetic field allows the pair\ncreation to happen with a smaller pitch angle.\n","num_tokens":390,"num_sentence":9}
{"input":"  Space-based gravitational-wave detectors, such as LISA or a similar ESA-led\nmission, will offer unique opportunities to test general relativity. We study\nthe bounds that space-based detectors could place on the graviton Compton\nwavelength \\lambda_g=h\/(m_g c) by observing multiple inspiralling black hole\nbinaries. We show that while observations of individual inspirals will yield\nmean bounds \\lambda_g~3x10^15 km, the combined bound from observing ~50 events\nin a two-year mission is about ten times better: \\lambda_g~3x10^16 km\n(m_g~4x10^-26 eV). The bound improves faster than the square root of the number\nof observed events, because typically a few sources provide constraints as much\nas three times better than the mean. This result is only mildly dependent on\ndetails of black hole formation and detector characteristics. The bound\nachievable in practice should be one order of magnitude better than this figure\n(and hence almost competitive with the static, model-dependent bounds from\ngravitational effects on cosmological scales), because our calculations ignore\nthe merger\/ringdown portion of the waveform. The observation that an ensemble\nof events can sensibly improve the bounds that individual binaries set on\n\\lambda_g applies to any theory whose deviations from general relativity are\nparametrized by a set of global parameters.\n","num_tokens":332,"num_sentence":7}
{"input":"  We derive one-shot upper bounds for quantum noisy channel codes. We do so by\nregarding a channel code as a bipartite operation with an encoder belonging to\nthe sender and a decoder belonging to the receiver, and imposing constraints on\nthe bipartite operation. We investigate the power of codes whose bipartite\noperation is non-signalling from Alice to Bob, positive-partial transpose (PPT)\npreserving, or both, and derive a simple semidefinite program for the\nachievable entanglement fidelity. Using the semidefinite program, we show that\nthe non-signalling assisted quantum capacity for memoryless channels is equal\nto the entanglement-assisted capacity. We also relate our PPT-preserving codes\nand the PPT-preserving entanglement distillation protocols studied by Rains.\nApplying these results to a concrete example, the 3-dimensional Werner-Holevo\nchannel, we find that codes that are non-signalling and PPT-preserving can be\nstrictly less powerful than codes satisfying either one of the constraints, and\ntherefore provide a tighter bound for unassisted codes. Furthermore,\nPPT-preserving non-signalling codes can send one qubit perfectly over two uses\nof the channel, which has no quantum capacity. We discuss whether this can be\ninterpreted as a form of superactivation of quantum capacity.\n","num_tokens":315,"num_sentence":8}
{"input":"  We investigate the ability of the Laser Interferometer Space Antenna (LISA)\nto measure the center of mass acceleration of stellar-origin black hole\nbinaries emitting gravitational waves. Our analysis is based on the idea that\nthe acceleration of the center of mass induces a time variation in the redshift\nof the gravitational wave, which in turn modifies its waveform. We confirm that\nwhile the cosmological acceleration is too small to leave a detectable imprint\non the gravitational waveforms observable by LISA, larger peculiar\naccelerations may be measurable for sufficiently long lived sources. We focus\non stellar mass black hole binaries, which will be detectable at low\nfrequencies by LISA and near coalescence by ground based detectors. These\nsources may have large peculiar accelerations, for instance, if they form in\nnuclear star clusters or in AGN accretion disks. If that is the case, we find\nthat in an astrophysical population calibrated to the LIGO-Virgo observed\nmerger rate, LISA will be able to measure the peculiar acceleration of a small\nbut significant fraction of the events if the mission lifetime is extended\nbeyond the nominal duration of 4 years. In this scenario LISA will be able to\nassess whether black hole binaries form close to galactic centers, particularly\nin AGN disks, and will thus help discriminate between different formation\nmechanisms. Although for a nominal 4 years LISA mission the peculiar\nacceleration effect cannot be measured, a consistent fraction of events may be\nbiased by strong peculiar accelerations which, if present, may imprint large\nsystematic errors on some waveform parameters. In particular, estimates of the\nluminosity distance could be strongly biased and consequently induce large\nsystematic errors on LISA measurements of the Hubble constant with stellar mass\nblack hole binaries.\n","num_tokens":438,"num_sentence":9}
{"input":"  Self-consistent, time-dependent supernova (SN) simulations in three spatial\ndimensions (3D) are conducted with the Aenus-Alcar code, comparing, for the\nfirst time, calculations with fully multi-dimensional (FMD) neutrino transport\nand the ray-by-ray-plus (RbR+) approximation, both based on a two-moment solver\nwith algebraic M1 closure. We find good agreement between 3D results with FMD\nand RbR+ transport for both tested grid resolutions in the cases of a 20\nsolar-mass progenitor, which does not explode with the employed simplified set\nof neutrino opacities, and of an exploding 9 solar-mass model. This is in stark\ncontrast to corresponding axisymmetric (2D) simulations, which confirm previous\nclaims that the RbR+ approximation can foster explosions in 2D in particular in\nmodels with powerful axial sloshing of the stalled shock due to the standing\naccretion shock instability (SASI). However, while local and instantaneous\nvariations of neutrino fluxes and heating rates can still be considerably\nhigher with RbR+ transport in 3D, the time-averaged quantities are very similar\nto FMD results because of the absence of a fixed, artificial symmetry axis that\nchannels the flow. Therefore, except for stochastic fluctuations, the neutrino\nsignals and the post-bounce evolution of 3D simulations with FMD and RbR+\ntransport are also very similar, in particular for our calculations with the\nbetter grid resolution. Higher spatial resolution has clearly a more important\nimpact than the differences by the two transport treatments. Our results back\nup the use of the RbR+ approximation for neutrino transport in 3D SN modeling.\n","num_tokens":421,"num_sentence":7}
{"input":"  It is shown that a broad class of cavity quantum electrodynamics (QED)\nproblems - which consider the resonant propagation of a single photon\ninteracting with quantum emitters (QEs), such as atoms, quantum dots, or\nvacancy centers - can be solved directly without application of the second\nquantization formalism. In the developed approach, the Hamiltonian is expressed\nthrough the ket-bra products of collective (photon + cavities + QEs) states.\nConsequently, the S-matrix of input-output problems is determined exactly by\nthe Mahaux-Weidenm\\\"uller formula, which dramatically simplifies the analysis\nof complex cavity QED systems. First, this approach is illustrated for the\nproblem of propagation of a photon resonantly interacting with N two-level QEs\narbitrary distributed inside the optical cavity. Solution of this problem\nmanifests the effect of cumulative action of QEs previously known for special\ncases. Can a similar cumulative action of QEs enhance the inelastic resonant\ntransmission of a single photon? We solve this problem for the case of an\noptical cavity having two modes resonantly coupled to electronic transitions of\nN three-level QEs. It is shown that the described structure is the simplest\nrealistic structure which enables the down-conversion of the single photon\nfrequency with the amplitude approaching unity in the absence of the external\ndriving field and for sufficiently small cavity losses and QE dissipation.\nOverall, the simplicity and generality of the developed approach suggest a\npractical way to identify and describe new phenomena in cavity QED.\n","num_tokens":374,"num_sentence":9}
{"input":"  We study colors and metallicities of the Brightest Cluster Galaxies (BCGs)\nand Intra-Cluster Light (ICL) in galaxy groups and clusters, as predicted by a\nsemi-analytic model of galaxy formation, coupled with a set of high-resolution\nN-body simulations. The model assumes stellar stripping and violent relaxation\nprocesses during galaxy mergers to be the main channels for the formation of\nthe ICL. We find that BCGs are more metal-rich and redder than the ICL, at all\nredshifts since the ICL starts to form ($z\\sim 1$). In good agreement with\nseveral observed data, our model predicts negative radial metallicity and color\ngradients in the BCG+ICL system. By comparing the typical colors of the ICL\nwith those of satellite galaxies, we find that mass and metals in the ICL come\nfrom galaxies of different mass, depending on the redshift. Stripping of low\nmass galaxies, $9<\\log M_* <10$, is the most important contributor in the early\nstage of the ICL formation, but the bulk of the mass\/metals contents are given\nby intermediate\/massive galaxies, $10<\\log M_* <11$, at lower redshift. Our\nanalysis supports the idea that stellar stripping is more important than galaxy\nmergers in building-up the ICL, and highlights the importance of\ncolors\/metallicity measurements for understanding the formation and evolution\nof the ICL.\n","num_tokens":349,"num_sentence":7}
{"input":"  Almost all statements in knowledge bases have a temporal scope during which\nthey are valid. Hence, knowledge base completion (KBC) on temporal knowledge\nbases (TKB), where each statement \\textit{may} be associated with a temporal\nscope, has attracted growing attention. Prior works assume that each statement\nin a TKB \\textit{must} be associated with a temporal scope. This ignores the\nfact that the scoping information is commonly missing in a KB. Thus prior work\nis typically incapable of handling generic use cases where a TKB is composed of\ntemporal statements with\/without a known temporal scope. In order to address\nthis issue, we establish a new knowledge base embedding framework, called\nTIME2BOX, that can deal with atemporal and temporal statements of different\ntypes simultaneously. Our main insight is that answers to a temporal query\nalways belong to a subset of answers to a time-agnostic counterpart. Put\ndifferently, time is a filter that helps pick out answers to be correct during\ncertain periods. We introduce boxes to represent a set of answer entities to a\ntime-agnostic query. The filtering functionality of time is modeled by\nintersections over these boxes. In addition, we generalize current evaluation\nprotocols on time interval prediction. We describe experiments on two datasets\nand show that the proposed method outperforms state-of-the-art (SOTA) methods\non both link prediction and time prediction.\n","num_tokens":315,"num_sentence":13}
{"input":"  Human lives are increasingly influenced by algorithms, which therefore need\nto meet higher standards not only in accuracy but also with respect to\nexplainability. This is especially true for high-stakes areas such as real\nestate valuation. Unfortunately, the methods applied there often exhibit a\ntrade-off between accuracy and explainability.\n  One explainable approach is case-based reasoning (CBR), where each decision\nis supported by specific previous cases. However, such methods can be wanting\nin accuracy. The unexplainable machine learning approaches are often observed\nto provide higher accuracy but are not scrutable in their decision-making.\n  In this paper, we apply evolutionary algorithms (EAs) to CBR predictors in\norder to improve their performance. In particular, we deploy EAs to the\nsimilarity functions (used in CBR to find comparable cases), which are fitted\nto the data set at hand. As a consequence, we achieve higher accuracy than\nstate-of-the-art deep neural networks (DNNs), while keeping interpretability\nand explainability.\n  These results stem from our empirical evaluation on a large data set of real\nestate offers where we compare known similarity functions, their EA-improved\ncounterparts, and DNNs. Surprisingly, DNNs are only on par with standard CBR\ntechniques. However, using EA-learned similarity functions does yield an\nimproved performance.\n","num_tokens":319,"num_sentence":12}
{"input":"  In quantum information, lifting is a systematic procedure that can be used to\nderive---when provided with a seed Bell inequality---other Bell inequalities\napplicable in more complicated Bell scenarios. It is known that the procedure\nof lifting introduced by Pironio [J. Math. Phys. A 46, 062112 (2005)] preserves\nthe facet-defining property of a Bell inequality. Lifted Bell inequalities\ntherefore represent a broad class of Bell inequalities that can be found in\n{\\em all} Bell scenarios. Here, we show that the maximal value of {\\em any}\nlifted Bell inequality is preserved for both the set of nonsignaling\ncorrelations and quantum correlations. Despite the degeneracy in the maximizers\nof such inequalities, we show that the ability to self-test a quantum state is\npreserved under these lifting operations. In addition, except for\noutcome-lifting, local measurements that are self-testable using the original\nBell inequality---despite the degeneracy---can also be self-tested using {\\em\nany} lifted Bell inequality derived therefrom. While it is not possible to\nself-test {\\em all} the positive-operator-valued measure elements using an\noutcome-lifted Bell inequality, we show that partial, but robust self-testing\nstatements on the underlying measurements can nonetheless be made from the\nquantum violation of these lifted inequalities. We also highlight the\nimplication of our observations on the usefulness of using lifted Bell-like\ninequalities as a device-independent witnesses for entanglement depth. The\nimpact of the aforementioned degeneracy on the geometry of the quantum set of\ncorrelations is briefly discussed.\n","num_tokens":404,"num_sentence":14}
{"input":"  This paper is the third one in a series, intended to investigate how the\nobserved kinematics of elliptical galaxies are affected by dust attenuation. In\nPaper I and Paper II, we investigated the effects of dust absorption; here we\nextend our modelling in order to include the effects of scattering. We describe\nhow kinematical information can be combined with the radiative transfer\nequation, and present a Monte Carlo code that can handle kinematical\ninformation in an elegant way.\n  Compared to the case where only absorption is taken into account, we find\nthat dust attenuation considerably affects the observed kinematics when\nscattering is included. For the central lines of sight, dust can either\ndecrease or increase the central observed velocity dispersion. The most\nimportant effect of dust attenuation, however, is found at large projected\nradii. The kinematics at these lines of sight are strongly affected by photons\nscattered into these lines of sight, which were emitted by high-velocity stars\nin the central regions of the galaxy. These photons bias the LOSVDs towards\nhigh line-of-sight velocities, and significantly increase the observed velocity\ndispersion and LOSVD shape parameters. These effects are similar to the\nexpected kinematical signature of a dark matter halo, such that dust\nattenuation may form an alternative explanation for the usual stellar\nkinematical evidence for dark matter halos around elliptical galaxies.\n  We apply our results to discuss several other topics in galactic dynamics,\nwhere we feel dust attenuation should be taken into account. In particular, we\nargue that the kinematics observed at various wavelengths can help to constrain\nthe spatial distribution of dust in elliptical galaxies.\n","num_tokens":405,"num_sentence":11}
{"input":"  We present a survey strategy to detect the neutral hydrogen (HI) power\nspectrum at $5<z<6$ using the SKA-Low radio telescope in presence of\nforegrounds and instrumental effects. We simulate observations of the\ninherently weak HI signal post-reionization with varying levels of noise and\ncontamination with foreground amplitudes equivalent to residuals after sky\nmodel subtraction. We find that blind signal separation methods on imaged data\nare required in order to recover the HI signal at large cosmological scales.\nComparing different methods of foreground cleaning, we find that Gaussian\nProcess Regression (GPR) performs better than Principle Component Analysis\n(PCA), with the key difference being that GPR uses smooth kernels for the total\ndata covariance. The integration time of one field needs to be larger than\n$\\sim 250$ h to provide large enough signal-to-noise ratio (SNR) to accurately\nmodel the data covariance for foreground cleaning. Images within the primary\nbeam field-of-view give measurements of the HI power spectrum at scales $k\\sim\n0.02\\,{\\rm Mpc^{-1}}-0.3\\,{\\rm Mpc^{-1} }$ with SNR $\\sim 2-5$ in $\\Delta[{\\rm\nlog}( k\/{\\rm Mpc^{-1}})] = 0.25$ bins assuming an integration time of $600$ h.\nSystematic effects, which introduce small-scale fluctuations across frequency\nchannels, need to be $\\lesssim 5\\times 10^{-5}$ to enable unbiased measurements\noutside the foreground wedge. Our results provide an important validation\ntowards using the SKA-Low array for measuring the HI power spectrum in the\npost-reionization Universe.\n","num_tokens":425,"num_sentence":9}
{"input":"  We analyze a low energy effective model of Dark Matter in which the thermal\nrelic density is provided by a singlet Majorana fermion which interacts with\nthe Higgs fields via higher dimensional operators. Direct detection signatures\nmay be reduced if blind spot solutions exist, which naturally appear in models\nwith extended Higgs sectors. Explicit mass terms for the Majorana fermion can\nbe forbidden by a $Z_3$ symmetry, which in addition leads to a reduction of the\nnumber of higher dimensional operators. Moreover, a weak scale mass for the\nMajorana fermion is naturally obtained from the vacuum expectation value of a\nscalar singlet field. The proper relic density may be obtained by the\n$s$-channel interchange of Higgs and gauge bosons, with the longitudinal mode\nof the $Z$ boson (the neutral Goldstone mode) playing a relevant role in the\nannihilation process. This model shares many properties with the\nNext-to-Minimal Supersymmetric extension of the Standard Model (NMSSM) with\nlight singlinos and heavy scalar and gauge superpartners. In order to test the\nvalidity of the low energy effective field theory, we compare its predictions\nwith those of the ultraviolet complete NMSSM. Extending our framework to\ninclude $Z_3$ neutral Majorana fermions, analogous to the bino in the NMSSM, we\nfind the appearance of a new bino-singlino well tempered Dark Matter region.\n","num_tokens":339,"num_sentence":8}
{"input":"  In this paper, we present the analytical theory of attosecond pulse formation\nvia optical modulation of an active medium of the hydrogen-like C5+\nplasma-based X-ray laser at 3.4 nm wavelength in the \"water window\" range,\ntaking into account a variation of the population inversion caused by radiative\ndecay of the upper lasing states. We derive an analytical solution for the\nX-ray field amplified by an X-ray laser with time-dependent population\ninversion, which is simultaneously irradiated by a strong optical laser field,\nand use it to find the optimal conditions for the attosecond pulse formation\nfrom a narrowband seeding X-ray field. We show that the shape of pulses can be\nimproved at the cost of reduced pulse peak intensity (i) via external\nattenuation of the resonant spectral component of the amplified X-ray field or\n(ii) by using a resonantly absorbing medium (the active medium of the X-ray\nlaser after the change of sign of the population inversion) for the pulse\nformation. The results of the analytical theory are in a good agreement with\nthe numerical solutions of the Maxwell-Bloch equations which account for the\nnonlinearity, as well as the amplified spontaneous emission, of the active\nmedium. Both analytically and numerically we show the possibility to produce a\ntrain of attosecond pulses with sub-200 as duration and the peak intensity\nexceeding 10^12 W\/cm^2 at the carrier wavelength 3.4 nm in the \"water window\"\nrange, which makes them attractive for the biological and medical applications.\n","num_tokens":393,"num_sentence":5}
{"input":"  We investigate the effects of gravitational lensing in the binary pulsar\nsystem J0737-3039. Current measurement of the orbital inclination allows the\nmillisecond pulsar (A) to pass very close (at R_{min}=4000 km) in projection to\nthe companion pulsar (B), with R_{min} comparable to the Einstein radius (2600\nkm). For this separation at the conjunction, lensing causes small (about 10%)\nmagnification of the pulsar A signal on a timescale of several seconds, and\ndisplaces the pulsar image on the sky plane by about 1200 km. More importantly,\nlensing introduces a correction (of several microsec) to the conventional\nShapiro delay formula used in pulsar timing analysis, and gives rise to a\ngeometric time delay together with the delays associated with the pulsar spin\nperiod. These lensing effects can influence the determination of the system\nparameters by both timing and scintillation studies. Given the current\nuncertainty in the orbital inclination, more extreme manifestations of lensing\n(e.g. magnification by a factor of several) are possible. We compare our\npredictions with the existing observations and discuss the possibility of\ndetecting gravitational lensing signatures in the system. The anomalously high\npoint in A's lightcurve close to superior conjunction might be caused by\ngravitational lensing.\n","num_tokens":347,"num_sentence":8}
{"input":"  During the evolution of rotating first stars, which initially consisted of\nonly hydrogen and helium, CNO elements may emerge to their surface. These stars\nmay therefore have winds that are driven only by CNO elements. We study weak\nwind effects (Gayley-Owocki heating and multicomponent effects) in stellar\nwinds of first generation stars driven purely by CNO elements. We apply our\nNLTE multicomponent models and hydrodynamical simulations. The multicomponent\neffects (frictional heating and decoupling) are important particularly for low\nmetallicity winds, but they influence mass loss rate only if they cause\ndecoupling for velocities lower than the escape velocity. The multicomponent\neffects also modify the feedback from first stars. As a result of the\ndecoupling of radiatively accelerated metals from hydrogen and helium, the\nfirst low-energy cosmic ray particles are generated. We study the interaction\nof these particles with the interstellar medium concluding that these particles\neasily penetrate the interstellar medium of a given minihalo. We discuss the\ncharging of the first stars by means of their winds. Gayley-Owocki heating,\nfrictional heating, and the decoupling of wind components occur in the winds of\nevolved low-metallicity stars and the solar metallicity main-sequence stars.\n","num_tokens":321,"num_sentence":10}
{"input":"  We consider a one--spatial dimensional tumour growth model [2, 3, 4] that\nconsists of three dependent variables of space and time: volume fraction of\ntumour cells, velocity of tumour cells, and nutrient concentration. The model\nvariables satisfy a coupled system of semilinear advection equation\n(hyperbolic), simplified linear Stokes equation (elliptic), and semilinear\ndiffusion equation (parabolic) with appropriate conditions on the\ntime-dependent boundary, which is governed by an ordinary differential\nequation. We employ a reformulation of the model defined in a larger, fixed\ntime-space domain to overcome some theoretical difficulties related to the\ntime-dependent boundary. This reformulation reduces the complexity of the model\nby removing the need to explicitly track the time-dependent boundary, but\nnonlinearities in the equations, noncoercive operators in the simplified Stokes\nequation, and interdependence between the unknown variables still challenge the\nproof of suitable a priori estimates. A numerical scheme that employs a finite\nvolume method for the hyperbolic equation, a finite element method for the\nelliptic equation, and a backward Euler in time--mass lumped finite element in\nspace method for the parabolic equation is developed. We establish the\nexistence of a time interval $(0,T_{\\ast})$ over which, using compactness\ntechniques, we can extract a convergent subsequence of the numerical\napproximations. The limit of any such convergent subsequence is proved to be a\nweak solution of the continuous model in an appropriate sense, which we call a\nthreshold solution. Numerical tests and justifications that confirm the\ntheoretical findings conclude the paper.\n","num_tokens":384,"num_sentence":8}
{"input":"  Observations have found black holes spanning ten orders of magnitude in mass\nacross most of cosmic history. The Kerr black hole solution is however\nprovisional as its behavior at infinity is incompatible with an expanding\nuniverse. Black hole models with realistic behavior at infinity predict that\nthe gravitating mass of a black hole can increase with the expansion of the\nuniverse independently of accretion or mergers, in a manner that depends on the\nblack hole's interior solution. We test this prediction by considering the\ngrowth of supermassive black holes in elliptical galaxies over\n$0<z\\lesssim2.5$. We find evidence for cosmologically coupled mass growth among\nthese black holes, with zero cosmological coupling excluded at 99.98%\nconfidence. The redshift dependence of the mass growth implies that, at\n$z\\lesssim7$, black holes contribute an effectively constant cosmological\nenergy density to Friedmann's equations. The continuity equation then requires\nthat black holes contribute cosmologically as vacuum energy. We further show\nthat black hole production from the cosmic star formation history gives the\nvalue of $\\Omega_{\\Lambda}$ measured by Planck while being consistent with\nconstraints from massive compact halo objects. We thus propose that stellar\nremnant black holes are the astrophysical origin of dark energy, explaining the\nonset of accelerating expansion at $z \\sim 0.7$.\n","num_tokens":322,"num_sentence":9}
{"input":"  We describe two complementary methods to quantify the degree of burial of\nligand and\/or ligand binding site (LBS) in a protein-ligand complex, namely,\nthe \"cutting plane\" (CP) and the \"tangent sphere\" (TS) methods. To construct\nthe CP and TS, two centroids are required: the protein molecular centroid\n(global centroid, GC), and the LBS centroid (local centroid, LC). The CP is\ndefined as the plane passing through the LBS centroid (LC) and normal to the\nline passing through the LC and the protein molecular centroid (GC). The\n\"anterior side\" of the CP is the side not containing the GC (which the\n\"posterior\" side does). The TS is defined as the sphere with center at GC and\ntangent to the CP at LC. The percentage of protein atoms (a.) inside the TS,\nand (b.) on the anterior side of the CP, are two complementary measures of\nligand or LBS burial depth since the latter is directly proportional to (b.)\nand inversely proportional to (a.). We tested the CP and TS methods using a\ntest set of 67 well characterized protein-ligand structures (Laskowski et al.,\n1996), as well as the theoretical case of an artificial protein in the form of\na cubic lattice grid of points in the overall shape of a sphere and in which\nLBS of any depth can be specified. Results from both the CP and TS methods\nagree very well with data reported by Laskowski et al., and results from the\ntheoretical case further confirm that that both methods are suitable measures\nof ligand or LBS burial. Prior to this study, there were no such numerical\nmeasures of LBS burial available, and hence no way to directly and objectively\ncompare LBS depths in different proteins. LBS burial depth is an important\nparameter as it is usually directly related to the amount of conformational\nchange a protein undergoes upon ligand binding, and ability to quantify it\ncould allow meaningful comparison of protein dynamics and flexibility.\n","num_tokens":487,"num_sentence":10}
{"input":"  We present the results of a study which uses the 3CRR sample of radio-loud\nactive galactic nuclei to investigate the evolution of the black-hole:spheroid\nmass ratio in the most massive early-type galaxies from 0<z<2. Radio-loud\nunification is exploited to obtain virial (line-width) black-hole mass\nestimates from the 3CRR quasars, and stellar mass estimates from the 3CRR radio\ngalaxies, thereby providing black-hole and stellar mass estimates for a single\npopulation of early-type galaxies. At low redshift (z<1) the 3CRR sample is\nconsistent with a black-hole:spheroid mass ratio of M_bh\/M_sph ~0.002, in good\nagreement with that observed locally for quiescent galaxies of similar stellar\nmass (M_sph ~5x10^11 M_sun). However, over the redshift interval 0<z<2 the 3CRR\nblack-hole:spheroid mass ratio is found to evolve as M_bh\/M_sph \\propto\n(1+z)^{2.07\\pm0.76}, reaching M_bh\/M_sph ~0.008 by redshift z~2. This evolution\nis found to be inconsistent with the local black hole:spheroid mass ratio\nremaining constant at a moderately significant level (98%). If confirmed, the\ndetection of evolution in the 3CRR black-hole:spheroid mass ratio further\nstrengthens the evidence that, at least for massive early-type galaxies, the\ngrowth of the central supermassive black hole may be completed before that of\nthe host spheroid.\n","num_tokens":417,"num_sentence":6}
{"input":"  XMM-Newton observations of the outskirts of the Coma cluster of galaxies\nconfirm the existence of a soft X-ray excess claimed previously and show it\ncomes from warm thermal emission. Our data provide a robust estimate of its\ntemperature (~0.2 keV) and oxygen abundance (~0.1 solar). Using a combination\nof XMM-Newton and ROSAT All-Sky Survey data, we rule out a Galactic origin of\nthe soft X-ray emission. Associating this emission with a 20 Mpc region in\nfront of Coma, seen in the skewness of its galaxy velocity distribution, yields\nan estimate of the density of the warm gas of ~50 f_baryon rho_critical, where\nf_baryon is the baryon fraction of the gas and rho_critical is the critical\ndensity needed to halt the expansion of the universe. Our measurement of the\ngas mass associated with the warm emission strongly support its nonvirialized\nnature, suggesting that we are observing the warm-hot intergalactic medium\n(WHIM). Our measurements provide a direct estimate of the O, Ne and Fe\nabundance of the WHIM. Differences with the reported Ne\/O ratio for some OVI\nabsorbers hints at a different origin of the OVI absorbers and the Coma\nfilament. We argue that the Coma filament has likely been preheated, but at a\nsubstantially lower level compared to what is seen in the outskirts of groups.\nThe thermodynamic state of the gas in the Coma filament reduces the\nstar-formation rate in the embedded spiral galaxies, providing an explanation\nfor the presence of passive spirals observed in this and other clusters.\n","num_tokens":396,"num_sentence":9}
{"input":"  Propagation of large amplitude ion-acoustic solitary waves (IASWs) in a fully\nrelativistic plasma consisting of cold ions and ultrarelativistic hot electrons\nand positrons is investigated using the Sagdeev's pseudopotential method in a\nrelativistic hydrodynamics model. Effects of streaming speed of plasma fluid,\nthermal energy, positron density and positron temperature on large amplitude\nIASWs are studied by analysis of the pseudopotential structure. It is found\nthat in regions that the streaming speed of plasma fluid is larger than that of\nsolitary wave, by increasing the streaming speed of plasma fluid the depth and\nwidth of potential well increases and resulting in narrower solitons with\nlarger amplitude. This behavior is opposite for the case where the streaming\nspeed of plasma fluid is smaller than that of solitary wave. On the other hand,\nincrease of the thermal energy results in wider solitons with smaller\namplitude, because the depth and width of potential well decreases in that\ncase. Additionally, the maximum soliton amplitude increases and the width\nbecomes narrower as a result of increase in positron density. It is shown that\nvarying the positron temperature does not have considerable effect on the width\nand amplitude of IASWs. The existence of stationary soliton-like arbitary\namplitude waves is also predicted in fully relativistic electron-positron-ion\n(EPI) plasmas. Effects of streaming speed of plasma fluid, thermal energy,\npositron density and positron temperature on these kinds of solitons are the\nsame as that for large amplitude IASWs.\n","num_tokens":380,"num_sentence":9}
{"input":"  The missing data issue is ubiquitous in health studies. Variable selection in\nthe presence of both missing covariates and outcomes is an important\nstatistical research topic but has been less studied. Existing literature\nfocuses on parametric regression techniques that provide direct parameter\nestimates of the regression model. Flexible nonparametric machine learning\nmethods considerably mitigate the reliance on the parametric assumptions, but\ndo not provide as naturally defined variable importance measure as the\ncovariate effect native to parametric models. We investigate a general variable\nselection approach when both the covariates and outcomes can be missing at\nrandom and have general missing data patterns. This approach exploits the\nflexibility of machine learning modeling techniques and bootstrap imputation,\nwhich is amenable to nonparametric methods in which the covariate effects are\nnot directly available. We conduct expansive simulations investigating the\npractical operating characteristics of the proposed variable selection\napproach, when combined with four tree-based machine learning methods, XGBoost,\nRandom Forests, Bayesian Additive Regression Trees (BART) and Conditional\nRandom Forests, and two commonly used parametric methods, lasso and backward\nstepwise selection. Numeric results suggest that when combined with bootstrap\nimputation, XGBoost and BART have the overall best variable selection\nperformance with respect to the $F_1$ score and Type I error across various\nsettings. In general, there is no significant difference in the variable\nselection performance due to imputation methods. We further demonstrate the\nmethods via a case study of risk factors for 3-year incidence of metabolic\nsyndrome with data from the Study of Women's Health Across the Nation.\n","num_tokens":387,"num_sentence":10}
{"input":"  The main goal of this paper is to prove the following: for a triangulated\ncategory $ \\underline{C}$ and $E\\subset \\operatorname{Obj} \\underline{C}$ there\nexists a cohomological functor $F$ (with values in some abelian category) such\nthat $E$ is its set of zeros if (and only if) $E$ is closed with respect to\nretracts and extensions (so, we obtain a certain Nullstellensatz for functors\nof this type). Moreover, for $ \\underline{C}$ being an $R$-linear category\n(where $R$ is a commutative ring) this is also equivalent to the existence of\nan $R$-linear $F: \\underline{C}^{op}\\to R-\\operatorname{mod}$ satisfying this\nproperty.\n  As a corollary, we prove that an object $Y$ belongs to the corresponding\n\"envelope\" of some $D\\subset \\operatorname{Obj} \\underline{C}$ whenever the\nsame is true for the images of $Y$ and $D$ in all the categories $\n\\underline{C}_p$ obtained from $ \\underline{C}$ by means of \"localizing the\ncoefficients\" at maximal ideals $p\\triangleleft R$. Moreover, to prove our\ntheorem we develop certain new methods for relating triangulated categories to\ntheir (non-full) countable triangulated subcategories.\n  The results of this paper can be applied to the study of weight structures\nand of triangulated categories of motives.\n","num_tokens":342,"num_sentence":7}
{"input":"  Optical tweezers microrheology (OTM) offers a powerful approach to probe the\nnonlinear response of complex soft matter systems, such as networks of\nentangled polymers, over wide-ranging spatiotemporal scales. OTM can also\nuniquely characterize the microstructural dynamics that lead to the intriguing\nnonlinear rheological properties that these systems exhibit. However, the\nstrain in OTM measurements, applied by optically forcing a micro-probe through\nthe material, induces network inhomogeneities in and around the strain path,\nand the resultant flow field complicates the measured response of the system.\nThrough a robust set of custom-designed OTM protocols, coupled with modeling\nand analytical calculations, we characterize the time-varying inhomogeneity\nfields induced by OTM measurements. We show that post-strain homogenization\ndoes not interfere with the intrinsic stress relaxation dynamics of the system,\nrather it manifests as an independent component in the stress decay, even in\nhighly nonlinear regimes such as with the microrheological-LAOS (mLAOS)\nprotocols we introduce. Our specific results show that Rouse-like elastic\nretraction, rather than disentanglement and disengagement, dominates the\nnonlinear stress relaxation of entangled polymers at micro- and meso- scales.\nThus, our study opens up possibilities of performing precision nonlinear\nmicrorheological measurements, such as mLAOS, on a wide range of complex\nmacromolecular systems.\n","num_tokens":362,"num_sentence":7}
{"input":"  We consider cold Rydberg atoms in a one-dimensional optical lattice in the\nMott regime with a single atom per site at zero temperature. An external laser\ndrive with Rabi frequency \\Omega and laser detuning \\Delta, creates Rydberg\nexcitations whose dynamics is governed by an effective spin-chain model with\n(quasi) long-range interactions. This system possesses intrinsically a large\ndegree of frustration resulting in a ground-state phase diagram in the\n(\\Delta,\\Omega) plane with a rich topology. As a function of \\Delta, the\nRydberg blockade effect gives rise to a series of crystalline phases\ncommensurate with the optical lattice that form a so-called devil's staircase.\nThe Rabi frequency, \\Omega, on the other hand, creates quantum fluctuations\nthat eventually lead to a quantum melting of the crystalline states. Upon\nincreasing \\Omega, we find that generically a commensurate-incommensurate\ntransition to a floating Rydberg crystal occurs first, that supports gapless\nphonon excitations. For even larger \\Omega, dislocations within the floating\nRydberg crystal start to proliferate and a second,\nKosterlitz-Thouless-Nelson-Halperin-Young dislocation-mediated melting\ntransition finally destroys the crystalline arrangement of Rydberg excitations.\nThis latter melting transition is generic for one-dimensional Rydberg crystals\nand persists even in the absence of an optical lattice. The floating phase and\nthe concomitant transitions can, in principle, be detected by Bragg scattering\nof light.\n","num_tokens":388,"num_sentence":9}
{"input":"  Rocky planets both in and outside of our solar system are observed to have a\nrange of core-mass fractions (CMFs). Imperfect collisions can preferentially\nstrip mantle material from a planet, changing its CMF, and are therefore\nthought to be the most likely cause of this observed CMF variation. However,\nprevious work that implements these collisions into N-body simulations of\nplanet formation has struggled to reliably form high CMF super-Earths. In this\nwork, we specify our initial conditions and simulation parameters to maximize\nthe prevalence of high-energy, CMF-changing collisions in order to form planets\nwith highly diverse CMFs. High-energy collisions have a large $v_{imp}\/v_{esc}$\nratio, so we maximize this ratio by starting simulations with high-eccentricity\nand inclination disks to increase the difference in their orbital velocities,\nmaximizing $v_{imp}$. Additionally, we minimize $v_{esc}$ by starting with\nsmall embryos. The final planets undergo more high-energy, debris-producing\ncollisions, and experience significant CMF change over their formation.\nHowever, we find that a number of processes work together to average out the\nCMF of a planet over time, therefore we do not consistently form high-CMF, high\nmass planets. We do form high-CMF planets below 0.5 $M_{\\oplus}$. Additionally,\nwe find in these highly eccentric environments, loss of debris mass due to\ncollisional grinding has a significant effect on final planet masses and CMFs,\nresulting in smaller planets and a higher average planet CMF. This work\nhighlights the importance of improving measurements of high-density planets to\nbetter constrain their CMFs.\n","num_tokens":424,"num_sentence":11}
{"input":"  Chaotic systems are highly sensitive to a small perturbation, and are\nubiquitous throughout biological sciences, physical sciences and even social\nsciences. Taking this as the underlying principle, we construct an operational\nnotion for quantum chaos. Namely, we demand that the future state of a\nmany-body, isolated quantum system is sensitive to past multitime operations on\na small subpart of that system. By `sensitive', we mean that the resultant\nstates from two different perturbations cannot easily be transformed into each\nother. That is, the pertinent quantity is the complexity of the effect of the\nperturbation within the final state. From this intuitive metric, which we call\nthe Butterfly Flutter Fidelity, we use the language of multitime quantum\nprocesses to identify a series of operational conditions on chaos, in\nparticular the scaling of the spatiotemporal entanglement. Our criteria already\ncontain the routine notions, as well as the well-known diagnostics for quantum\nchaos. This includes the Peres-Loschmidt Echo, Dynamical Entropy, Tripartite\nMutual Information, and Local-Operator Entanglement. We hence present a unified\nframework for these existing diagnostics within a single structure. We also go\non to quantify how several mechanisms lead to quantum chaos, such as evolution\ngenerated from random circuits. Our work paves the way to systematically study\nmany-body dynamical phenomena like Many-Body Localization, measurement-induced\nphase transitions, and Floquet dynamics.\n","num_tokens":349,"num_sentence":11}
